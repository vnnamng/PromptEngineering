# 5.7. Prompt Engineering

Chat language models, or chat LMs, are language models finetuned on dialogue examples. This finetuning resembles instruction finetuning but uses multi-turn conversation inputs, such as those in the ChatML format, with the targets being the assistant’s responses.

Despite its simplicity, the conversational interface allows solving various practical problems. This section explores best practices for using chat LMs to address such problems known as prompt engineering techniques.

## 5.7.1. Features of a Good Prompt

To get the best results from a chat LM, you need a well-crafted prompt. The key components of a strong prompt include:

1. **Situation**: Describe why you’re asking for help.
2. **Role**: Define the expert persona the model should emulate.
3. **Task**: Give clear, specific instructions about what the model must do.
4. **Output format**: Explain how you expect the response to be structured, such as bullet points, JSON, or code.
5. **Constraints**: Mention any limitations, preferences, or requirements.
6. **Quality criteria**: Define what makes a response satisfactory.
7. **Examples**: Provide few-shot examples of inputs with expected outputs.
8. **Call to action**: Restate the task simply and ask the model to perform it.

Putting input-output examples in the prompt is called few-shot prompting or in-context learning. These examples include both positive cases showing desired outputs and negative ones demonstrating incorrect responses. Adding explanations that connect incorrect responses to specific constraints helps the model understand why they are wrong.

Here’s an example of a prompt that includes some of the above elements:

````markdown
Situation: I'm creating a system to analyze insurance claims. It processes adjuster reports to extract key details for display in a SaaS platform.

Your role: Act as a seasoned insurance claims analyst familiar with industry-standard classifications.

Task: Identify the type of incident, the primary cause, and the significant damages described in the report.

Output format: Return a JSON object with this structure:
```json
{
  "type": "string",   // Incident type
  "cause": "string",  // Primary cause
  "damage": ["string"]  // Major damages
}
````

<examples>
<example>
<input>
Observed two-vehicle accident at an intersection. Insured's car was hit after the other driver ran a red light. Witnesses confirm. The vehicle has severe front-end damage, airbags deployed, and was towed from the scene.
</input>
<output>
```json
{
  "type": "collision",
  "cause": "failure to stop at signal",
  "damage": ["front-end damage", "airbag deployment"]
}
```
</output>
</example>
<example>
...
</example>
</examples>

Call to action: Extract the details from this report:

> "Arrived at the scene of a fire at a residential building. Extensive damage to the kitchen and smoke damage throughout. Fire caused by unattended cooking. Neighbors evacuated; no injuries reported."

Section names such as “Situation,” “Your role,” or “Task” are optional. When working on a prompt, keep in mind that the attention mechanism in LLMs has limitations. It might concentrate on certain parts of a prompt while overlooking others. A good prompt strikes a balance between detail and brevity. Excessive detail can overwhelm the model, while insufficient detail risks leaving gaps that the model may fill with incorrect assumptions.

I used XML tags for few-shot examples because they clearly define example boundaries and are familiar to LLMs from pretraining on structured data. Furthermore, chat LM models are often finetuned using conversational examples with XML structures. Using XML isn’t mandatory though, but could be helpful.

## 5.7.2. Followup Actions

The first solution from a model is often imperfect. User analysis and follow-up are key to getting the most out of a chat LM. Common follow-up actions include:

1. Asking the LLM whether its solution contains errors or can be simplified without breaking the constraints.
2. Copying the solution and starting a new conversation from scratch with the same LLM. In this new conversation, the user can ask the model to validate the solution as if it were “provided by an expert,” without revealing it was generated by the same model.
3. Using a different LLM to review or enhance the solution.
4. For code outputs, running the code in the execution environment, analyzing the results, and giving feedback to the model. If code fails, the full error message and stack traceback can be shared with the model.

When working with the same chat LM for follow-ups, especially in tasks like coding or handling complex structured outputs, it’s generally a good idea to start fresh after three–five exchanges. This recommendation comes from two key observations:

1. Chat LMs are typically finetuned using examples of short conversations. Creating long, high-quality conversations for finetuning is both difficult and costly, so the training data often lacks examples of long interactions focused on problem solving. As a result, the model performs better with shorter exchanges.
2. Long contexts can cause errors to accumulate. In the self-attention mechanism, the softmax is applied over many positions to compute weights for combining value vectors. As the context length increases, inaccuracies build up, and the model’s “focus” may shift to irrelevant details or earlier mistakes.

When starting fresh, it’s important to update the initial prompt with key details from earlier follow-ups. This helps the model avoid repeating previous mistakes. By consolidating the relevant information into a clear, concise starting point, you ensure the model has the context it needs without relying on the long and noisy history of the prior conversation.

## 5.7.3. Code Generation

One valuable use of chat LMs is generating code. The user describes the desired code, and the model tries to generate it. As we know, modern LLMs are pretrained on vast collections of open-source code across many programming languages. This pretraining allows them to learn syntax and many standard or widely used libraries. Seeing the same algorithms implemented in different languages also enables LLMs to form shared internal representations (like synonyms in word2vec), making them generally indifferent to the programming language when reading or creating code.

Moreover, much of this code includes comments and annotations, which help the model understand the code’s purpose—what it is designed to achieve. Sources like StackOverflow and similar forums add further value by providing examples of problems paired with their solutions. The exposure to such data gave LLMs an ability to respond with relevant code. Supervised finetuning improved their skill in interpreting user requests and turning them into code.

As a result, LLMs can generate code in nearly any language. For high-quality results, users must specify in detail what code should do. For example, providing a detailed docstring like this:

```python
def find_target_sum(numbers: list[int], target: int) -> tuple[int, int] | None:
    """
    Find pairs of indices in a list whose values sum to a target.
    
    Args:
        numbers: List of integers to search through. Can be empty.
        target: Integer sum to find.
    
    Returns:
        Tuple of two distinct indices whose values sum to target,
        or None if no solution exists.
    
    Examples:
        >>> find_target_sum([2, 7, 11, 15], 9)
        (0, 1)
        >>> find_target_sum([3, 3], 6)
        (0, 1)
        >>> find_target_sum([1], 5)
        None
        >>> find_target_sum([], 0)
        None
    
    Requirements:
    - Time complexity: O(n)
    - Space complexity: O(n)
    - Each index can only be used once
    - If multiple solutions exist, return any valid solution
    - All numbers and target can be any valid integer
    - Return None if no solution exists
    """
    ...
````

Providing a highly detailed docstring can sometimes feel as time-consuming as coding the function itself. A less detailed description might seem more practical, but this increases the likelihood of the generated code not fully meeting user needs. In such cases, users can review the output and refine their instructions with additional requests or constraints.

By the way, the book’s official website, [thelmbook.com](https://thelmbook.com), was created entirely through collaboration with an LLM. While it wasn’t generated perfectly on the first try, through iterative feedback, multiple conversation restarts, and switching between different chat LMs when needed, I refined every element you see—from the graphics to the animations—until they met my vision.

Language models can generate functions, classes, or even entire applications. However, the chance of success decreases as the level of abstraction increases. If the problem resembles the model’s training data, the model performs well with minimal input. However, for novel or unique business or engineering problems, detailed instructions are crucial for good results.

If you decide to use a brief prompt to save time, ask the model to pose clarifying questions. You can also request it to describe the code it plans to generate first. This allows you to adjust or add details to the instructions before code is created.
